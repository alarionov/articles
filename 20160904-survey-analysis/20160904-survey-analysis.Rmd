---
title: "Survey analysis - General Study"
author: "Artem Larionov"
date: "September 4, 2016"
output: html_document
abstract: The purpose of this article is to show how simple visualization techniques can be used to better understand trends evident in data gathered from employee surveys and determine directions for future analysis. Typically, such surveys give respondents a few choices when answering a question by simply clicking on a relevant box. In contrast, open-ended surveys, on which this article is based, ask respondents to answer questions in their own words. Here, I've applied a few techniques of text analysis to interpret open ended answers from surveys. 

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE)
setwd('~/articles/20160904-survey-analysis')

library(dplyr)
library(syuzhet)
library(tm)
library(RWeka)
library(SnowballC)  
library(stringr)
library(wordcloud)
library(RColorBrewer)
library(plyr)
library(ggplot2)
library(easyGgplot2)

prepareString <- function (x) {
  x <- removeWords(x, stopwords())
  x <- iconv(x, 'latin1', 'ASCII', sub='')
  x <- tolower(x)
  x <- removePunctuation(x)
  x <- removeNumbers(x)
  x <- stripWhitespace(x)
  str_trim(x)
}

buildDTMatrix <- function (corpus, min, max, language = 'english', weighting = weightTf) {
  DocumentTermMatrix(
    corpus, 
    control = list(
      tokenize = function(x) {
        NGramTokenizer(x, Weka_control(min = min, max = max))
      },
      language = language,
      stemWords = FALSE,
      wordLengths=c(3,Inf),
      weighting = weighting
    )
  )
}

question.text <- function (id) { questions[questions$id == id,]$body }
topic.id      <- function (id) { questions[questions$id == id,]$topic_id }
topic.title   <- function (id) { topics[topics$id == id,]$title } 

topics    <- read.csv('topics.csv', stringsAsFactors = FALSE)
questions <- read.csv('questions.csv', stringsAsFactors = FALSE) 
answers   <- read.csv('answers.csv', stringsAsFactors = FALSE)

answers <- subset(answers, company_id == 3)
answers$question <- sapply(answers$poll_id, question.text)
answers$topic_id <- as.factor(sapply(answers$poll_id, topic.id))
answers$topic    <- sapply(answers$topic_id, topic.title)
answers$answer_sentiment <- get_nrc_sentiment(answers$text)[1:8]
answers$question_sentiment <- get_nrc_sentiment(answers$question)[1:8]

poll_87 <- subset(answers, poll_id == 87)
poll_32 <- subset(answers, poll_id == 32)
poll_59 <- subset(answers, poll_id == 59)
poll_62 <- subset(answers, poll_id == 62)

custom_questions <- subset(questions, topic_id == 7 & company_id == 3)
```

# Introduction
In this article, I focus specifically on analyzing word frequencies and sentiments to understand tendency in open-ended responses. I use a base plotting system of R for visualization of word frequencies and the “syuzhet” package to retrieve sentiments. The implementation of the analysis itself won't be the main issue here, but if you are interested in this sub-topic, you can find the source code under this [link](https://github.com/alarionov/articles/blob/gh-pages/20160904-survey-analysis/20160904-survey-analysis.Rmd).

# Word clouds
Word clouds (aka tag clouds) are a visual representation of text data used mainly to characterize keyword metadata (tags) on websites or to visualize free form text. They depict the word frequency in a given text as a weighted list. More information on word clouds can be found [here](https://en.wikipedia.org/wiki/Tag_cloud).
Below you can see word clouds for our questions and answers:

```{r}
par(mfrow=c(1,2))
wordcloud(answers$question, random.order = F, colors = brewer.pal(6, 'Dark2'), main = 'Word Cloud for Questions')
wordcloud(answers$text, random.order = F, colors = brewer.pal(6, 'Dark2'), main = 'Word Cloud for Answers')
```

As we could expect, the questions are focused on "what" and "how" about "company" and "work".
Answers look quite positive with a big "good" in the middle, the biggest "know" is probably related to "don't" right above it. It might be useful to find those answers and questions to understand whether the employees don't know something important, or just haven't gotten familiar with the survey tool yet. 

# Word frequency
Word frequency can be useful for certain types of questions because it summarizes all answers into the most common words. Firstly, all answers are being split into separate words. Secondly, we count the frequency of each word. And thirdly, we plot a bar where we show the frequency of each and every word from the text, sorted from the most to the least frequent. Thanks to word frequency method we can see the most frequently used words and trends behind even the biggest number of answers. However, it also ignores the context of the word so, for example, it doesn't understand and show the difference between "very good" and "not very good".

```{r}
corpus <- Corpus(VectorSource(paste(poll_62$text, collapse = ' ')))
corpus <- tm_map(corpus, prepareString)
corpus <- tm_map(corpus, PlainTextDocument)
unigram_sparse_dtm <- buildDTMatrix(corpus, 1, 1)
matrix <- as.matrix(unigram_sparse_dtm)
uni_sorted <- matrix[1,order(matrix[1,])]
par(mar=c(5,5,4,2) + 0.1, mfrow=c(1,1))
barplot(tail(uni_sorted, n =10), horiz = TRUE, cex.names = 0.7, las = 1, main = questions[questions$id == 62,]$body)
```

For instance, in the graph above the word "disturb" actually has positive meaning, because it was used with negation, so it's always good to check what is hiding behind the numbers. 

```{r}
poll_62$text[grep('disturb', poll_62$text)]
```

# Sentiments
According to Plutchik's theory, there are eight basic emotions: anger, anticipation, disgust, fear, joy, sadness, surprise and trust.
"Syuzhet" package provides a functionality to check if a word or a sentence is related to these emotions. It also says if it's positive or negative.  

```{r}
sentiments <- get_nrc_sentiment(c('good', 'friendly', 'work', 'just', 'fine'))
rownames(sentiments) <- c('good', 'friendly', 'work', 'just', 'fine')
sentiments
```

See the examples of sentiment graphs below.

```{r}
par(mfrow = c(2, 1))
barplot(sort(colSums(poll_62$answer_sentiment)), horiz = TRUE, cex.names = 0.7, las = 1, main= questions[questions$id == 62,]$body)
barplot(sort(colSums(answers$answer_sentiment)), horiz = TRUE, cex.names = 0.7, las = 1, main="Commulative sentiments for all answers")
```

To demonstrate what words from the provided answers are related to each emotion, we can combine word frequencies and sentiments.

```{r}
corpus <- Corpus(VectorSource(paste(answers$text, collapse = ' ')))
corpus <- tm_map(corpus, prepareString)
corpus <- tm_map(corpus, PlainTextDocument)
unigram_sparse_dtm <- buildDTMatrix(corpus, 1, 1)
matrix <- as.matrix(unigram_sparse_dtm)
uni_sorted <- matrix[1,order(matrix[1,])]
sentiments <- get_nrc_sentiment(names(uni_sorted)) * uni_sorted
sentiments <- sentiments[1:8]

for (i in 1:nrow(sentiments)) {
  row = sentiments[i,]
  if (sum(row) > 0) {
    row = row / sum(row)
    sentiments[i,] = row 
  }
}

indicies = list()
for (name in colnames(sentiments)) {
  indicies[[name]] <- as.numeric(rownames(sentiments[sentiments[name] >= 0.5,]))
}

par(mfrow=c(3,3))
for (name in names(indicies)) {
  if (length(indicies[[name]])) {
    barplot(tail(uni_sorted[indicies[[name]]], n = 5), horiz = T,cex.names = 0.7, las = 1, main = name)
  }
}
```

Sentiment graph allows to see the common trend behind the big amount of text, but it also ignores the context so it doesn’t show irony, sarcasm or negation.

# Sentiments and time series
Even though the answers seem mostly positive, they may keep changing over time (see the graphs below).

```{r}
sentiments_by_survey <- do.call(
  rbind.data.frame, 
  by(
    answers, 
    answers$survey_template_session_id, 
    function(x) colMeans(x$answer_sentiment), 
    simplify = F
  )
)
colnames(sentiments_by_survey) <- colnames(answers$answer_sentiment)
par(mfrow=c(2,4))
for (i in 1:8) {
  plot(
    1:nrow(sentiments_by_survey),
    sentiments_by_survey[[i]], 
    type = 'l',
    cex.names = 0.7,
    xlab = '# of surveys',
    ylab = colnames(sentiments_by_survey)[i],
    main = colnames(sentiments_by_survey)[i]
  )
}
```

# Conlusion
These techniques allow to understand common trends and could be used as a summary/overview of а big amount of text, but they also require a detailed analysis as to the context.